{"cells":[{"cell_type":"markdown","metadata":{"id":"t2rd2GYlnYGX"},"source":["# 한국어 혐오 발언 탐지"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20790,"status":"ok","timestamp":1693462464751,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"b4YkgbEocsAb","outputId":"d150072e-7b5c-4acd-a6db-6646ed24b2c6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZaCnbT5Reaqf"},"source":["## 필요 라이브러리 설치"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70293,"status":"ok","timestamp":1693473244783,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"680tRbdhebLg","outputId":"fd3d8939-98c9-4846-ea74-7e599c3dde87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.3)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.4)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\n","Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.3)\n","Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (23.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.2.1)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.2.1)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.4.2)\n","Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n","Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n","Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-frfkvwma/kobert-tokenizer_3337928fb18340dcb738111c1d2e5dd9\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-frfkvwma/kobert-tokenizer_3337928fb18340dcb738111c1d2e5dd9\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers\n","!pip install datasets\n","!pip install sentencepiece\n","!pip install torchmetrics\n","!pip install accelerate -U\n","!pip install scikit-learn\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n"]},{"cell_type":"markdown","metadata":{"id":"Qz98cJCasPgI"},"source":["## 데이터셋 로드"]},{"cell_type":"markdown","metadata":{"id":"qqO0w2cKsjN1"},"source":["- 학습, 검증, 테스트 데이터셋 준비\n","- 라벨 정보\n","\n","      class_label:\n","        names:\n","          0: origin\n","          1: physical\n","          2: politics\n","          3: profanity\n","          4: age\n","          5: gender\n","          6: race\n","          7: religion\n","          8: not_hate_speech"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"-IczFOCZZfvx","outputId":"8318274e-bf48-48df-eb13-d661c1d6b94d"},"outputs":[],"source":["from datasets import load_dataset\n","\n","train = load_dataset(\"jeanlee/kmhas_korean_hate_speech\", split=\"train\")\n","validation = load_dataset(\"jeanlee/kmhas_korean_hate_speech\", split=\"validation\")\n","test = load_dataset(\"jeanlee/kmhas_korean_hate_speech\", split=\"test\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1853,"status":"ok","timestamp":1693473259520,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"wqHX7ImG54T6","outputId":"b927813c-2149-4d8c-d80d-df30f9049ac2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 78977\n","})\n","Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 8776\n","})\n","Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 21939\n","})\n","\"자한당틀딱들.. 악플질 고만해라.\"\n","[2, 4]\n"]}],"source":["# 데이터 예제 출력\n","\n","print(train)\n","print(validation)\n","print(test)\n","print(train['text'][0])\n","print(train['label'][0])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1693473259520,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"eo9Y7N2CsXC6","outputId":"5173db4e-a0d2-4a99-c38a-9dd786893b2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\"자한당틀딱들.. 악플질 고만해라.\"', '정치적으로 편향된 평론한은 분은 별로...', '적당히좀 쳐먹지.그랬냐??? 안그래도 문재인 때문에 나라 엉망진창인데...', '\"안서는 아재들 풀발기 ㅋㄲㅋ\"', '우와 ㅋ 능력자', '맛녀석 콩트보다 약했음맛녀석 애청자로써 70%실력발휘', '주영훈 솔직히 호감임 잉꼬부부로 소문났잖아', '이게주간아이돌이랑머가달라...', '아오 슈박 회사생활도 졑깥고 돈벌기 힘들어 죽겠구만 뭔 저딴것들 자꾸 tv나와서 사람 짜증나게하냐 외국서 편히살려면 아닥하고 살아라 대한민국서 취미로 돈벌어가지말고 좀 끄지라고!', '\"문재인 하는게 뭐 별거있냐?ㅂㅅㅅㅋ가 하는짓인데 어련하겠어.ㅋㅋㅋ\"']\n","{'text': ['\"자한당틀딱들.. 악플질 고만해라.\"', '정치적으로 편향된 평론한은 분은 별로...', '적당히좀 쳐먹지.그랬냐??? 안그래도 문재인 때문에 나라 엉망진창인데...', '\"안서는 아재들 풀발기 ㅋㄲㅋ\"', '우와 ㅋ 능력자', '맛녀석 콩트보다 약했음맛녀석 애청자로써 70%실력발휘', '주영훈 솔직히 호감임 잉꼬부부로 소문났잖아', '이게주간아이돌이랑머가달라...', '아오 슈박 회사생활도 졑깥고 돈벌기 힘들어 죽겠구만 뭔 저딴것들 자꾸 tv나와서 사람 짜증나게하냐 외국서 편히살려면 아닥하고 살아라 대한민국서 취미로 돈벌어가지말고 좀 끄지라고!', '\"문재인 하는게 뭐 별거있냐?ㅂㅅㅅㅋ가 하는짓인데 어련하겠어.ㅋㅋㅋ\"'], 'label': [[2, 4], [8], [2], [4], [8], [8], [8], [8], [3], [2, 3]]}\n"]}],"source":["# 사전\n","print(train['text'][0:10])\n","\n","# 리스트\n","print(train[0:10])\n"]},{"cell_type":"markdown","metadata":{"id":"R_2uWy1p8Ifx"},"source":["## 모델 및 토크나이저 로드"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":5145,"status":"error","timestamp":1693917591046,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"jnbQNuzk7rZR","outputId":"f19e3aee-c3c4-482b-b33b-27332c27cd11"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d76dfe6b468d4d87a164cbfb38de221c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38be2bf31304dbc966e33f01ef069ad","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c9169f7b5544aa9b21a5c60a2bba5e7","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/104k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24a402a97e6b4578808c876d07d3ba8f","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/397M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(16424, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","name_cards= ['monologg/koelectra-base-v3-discriminator', 'skt/kobert-base-v1', 'bert-base-multilingual-cased', 'snunlp/KR-BERT-char16424']\n","name_card = \"snunlp/KR-BERT-char16424\"\n","num_labels = 9\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if name_card == \"skt/kobert-base-v1\":\n","    from kobert_tokenizer import KoBERTTokenizer\n","    tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(name_card, do_lower_case=False, use_fast=False)\n","# print(tokenizer.pad_token_id)\n","model = AutoModelForSequenceClassification.from_pretrained(name_card, num_labels=num_labels, problem_type=\"multi_label_classification\")\n","model.to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jTUT0OSy5mm-","outputId":"e7ba0517-c6a4-4343-e995-f6ff01914639"},"outputs":[{"data":{"text/plain":["Model(\n","  (model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(16424, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (n_classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (intent_loss): FocalLoss()\n","  (n_loss): CrossEntropyLoss()\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn as nn\n","from transformers import AutoModel\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=1):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","        return F_loss.mean()\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.model = AutoModel.from_pretrained(name_card)\n","\n","        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n","        self.n_classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n","\n","        # self.intent_loss = nn.BCEWithLogitsLoss()\n","        self.intent_loss = FocalLoss()\n","        self.n_loss = nn.CrossEntropyLoss()\n","\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None, n_label=None):\n","        model_input= {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","        }\n","\n","        if token_type_ids is not None:\n","            model_input['token_type_ids'] = token_type_ids\n","\n","        hidden = self.model(**model_input)\n","\n","        # some output dont have pooler_output\n","        if hasattr(hidden, 'pooler_output'):\n","            cls = hidden.pooler_output\n","        else:\n","            cls = hidden.last_hidden_state[:, 0, :]\n","\n","        hidden = hidden.last_hidden_state[:, 1:, :]\n","\n","        intent_logit = self.classifier(hidden.mean(dim=1))\n","        n_logit = self.n_classifier(cls)\n","\n","        intent_loss = self.intent_loss(intent_logit, labels)\n","        num_loss = self.n_loss(n_logit, n_label)\n","\n","        loss = intent_loss + num_loss\n","\n","        model_output = {\n","            'loss': loss,\n","            'intent_logit': intent_logit,\n","            'n_logit': n_logit,\n","        }\n","\n","        return model_output\n","\n","\n","model = Model()\n","model.to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1693464855626,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"67p9XNOm-K12","outputId":"713fe8a9-7638-403f-86bf-f7920399f60d"},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', '\"', '자', '##한', '##당', '##틀', '##딱', '##들', '.', '.', '악', '##플', '##질', '고', '##만', '##해', '##라', '.', '\"', '[SEP]']\n","[2, 11, 280, 20, 146, 1688, 8152, 283, 5, 5, 1043, 1370, 310, 37, 46, 39, 79, 5, 11, 3]\n","{'input_ids': [2, 11, 280, 20, 146, 1688, 8152, 283, 5, 5, 1043, 1370, 310, 37, 46, 39, 79, 5, 11, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["# 토크나이저 예시\n","\n","ids = tokenizer.encode(train['text'][0])\n","tokenized_words = tokenizer.convert_ids_to_tokens(ids)\n","model_input = tokenizer(train['text'][0])\n","\n","print(tokenized_words)\n","print(ids)\n","print(model_input)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1693465554951,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"ig5R7bPUw8br","outputId":"0c8e4d33-c42f-47fe-814b-55bb8a96115f"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [2, 11, 280, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n","['[CLS]', '\"', '자', '[SEP]']\n"]}],"source":["# 토크나이저 예시2\n","\n","model_input = tokenizer(train['text'][0], max_length=4, truncation=True, padding=\"max_length\")\n","print(model_input)\n","reverse = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n","print(reverse)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1298,"status":"ok","timestamp":1693465557105,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"PN25M6wXxhus","outputId":"1d5b61c5-3fdb-4010-8a5b-42595cf9db93"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [2, 11, 280, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n","{'input_ids': [0, 113, 43998, 2], 'attention_mask': [1, 1, 1, 1]}\n","{'input_ids': [101, 1000, 100, 102], 'attention_mask': [1, 1, 1, 1]}\n"]}],"source":["# 토크나이저 예시3\n","\n","print(model_input)\n","\n","another_name_card = 'roberta-base'\n","another_tokenizer = AutoTokenizer.from_pretrained(another_name_card, do_lower_case=False)\n","model_input = another_tokenizer(train['text'][0], max_length=4, truncation=True, padding=\"max_length\")\n","print(model_input)\n","\n","another_name_card = 'distilbert-base-uncased'\n","another_tokenizer = AutoTokenizer.from_pretrained(another_name_card, do_lower_case=False)\n","model_input = another_tokenizer(train['text'][0], max_length=4, truncation=True, padding=\"max_length\")\n","print(model_input)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"5m9abTda-TIX"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def tokenize_function(examples):\n","    model_input = tokenizer(examples['text'], max_length=128, truncation=True, padding=\"max_length\")\n","\n","    mlb = MultiLabelBinarizer(classes=[0,1,2,3,4,5,6,7,8])\n","    one_hot_labels = mlb.fit_transform(examples['label'])\n","\n","    model_input['label'] = one_hot_labels\n","    # model_input['n_label'] = []\n","    # for one_hot_label in one_hot_labels:\n","    #     if one_hot_label[-1] == 0:\n","    #         model_input['n_label'].append(sum(one_hot_label) - 1)\n","    #     else:\n","    #         model_input['n_label'].append(0)\n","    model_input['n_label'] = [sum(one_hot_label[:-1]) for one_hot_label in one_hot_labels] # 이거도 가능할 듯\n","    # model_input['n_label'] = [sum(one_hot_label) - 1 for one_hot_label in one_hot_labels]\n","\n","    return model_input"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195,"referenced_widgets":["0912b330f7354f9f95c60a87f8caf994","8a4e9fa141b34dddba9ce934bfb3133d","ebe85e2f525747c2bab1059d40917cf6","57342a2d55b74c3b9a69a1805f94889c","5097178ceeaa4bfca166641758ad2fc4","6cf048b7a36a47c7a23f8eab7397d481","538a4c1eed7e40d29234c7120a3761b6","c92d88c8388541e29bedf12c03bc9bd7","a537a5456f44480b8e94dad15ea64b11","b26ac6c874384214a6e05402e16216af","cedd6f11236a49518a88510e7a81bd56"]},"executionInfo":{"elapsed":28200,"status":"ok","timestamp":1693473289678,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"OjnfLtIVIrIW","outputId":"7de4c6a9-bf5a-4e20-f345-89653d5262ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 78977\n","})\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d258e8cf8904c2398805cc2e94613b3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/78977 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fccd2ffa26f4f2cbb34b5e9a375d8d9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/8776 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask', 'n_label'],\n","    num_rows: 78977\n","})\n"]}],"source":["print(train)\n","\n","tokenized_train = train.map(tokenize_function, batched=True)\n","tokenized_valid = validation.map(tokenize_function, batched=True)\n","\n","print(tokenized_train)"]},{"cell_type":"markdown","metadata":{"id":"61-IoYan9kAq"},"source":["## 모델 학습"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-WqZr7Mz7rXD"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","BATCH_SIZE = 64 # 32\n","EPOCHS = 20\n","\n","save_dir = '/root/korean_hate_speech_detection/model'\n","\n","training_args = TrainingArguments(\n","    output_dir=save_dir,\n","    do_train=True,\n","    do_eval=True,\n","    # save_steps=999999999,\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    logging_strategy=\"epoch\",\n","    logging_dir='./logs',\n","    learning_rate=2e-5,\n","    run_name=\"v1\",\n","    save_strategy=\"no\",\n","    label_names=['labels', 'n_label'],\n","    seed=42,\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"lRE8aPEzQPrL"},"outputs":[],"source":["import torch\n","from torchmetrics import Accuracy, F1Score, HammingDistance, AUROC, ExactMatch\n","\n","def compute_metrics(eval_pred):\n","    threshold = 0.5\n","\n","    # model_output = eval_pred.predictions\n","    # labels = eval_pred.label_ids\n","    intent_logits, n_logits = eval_pred.predictions\n","    intent_labels, n_labels = eval_pred.label_ids\n","\n","\n","    intent_logits = torch.Tensor(intent_logits)\n","    intent_labels = torch.Tensor(intent_labels).long()\n","\n","    n_logits = torch.Tensor(n_logits)\n","    n_labels = torch.Tensor(n_labels).long()\n","\n","    n_pred = torch.argmax(n_logits, dim=1)\n","\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(intent_logits))\n","\n","    preds = torch.zeros(size = probs.size())\n","    preds[probs >= threshold] = 1\n","\n","    ...\n","\n","    for b in range(preds.shape[0]):\n","\n","        # hate: 1, no_hate: 1\n","        if preds[b][8] == 1:\n","            preds[b][:-1] = 0\n","\n","        # hate: 0, no_hate: 0\n","        if preds[b][:-1].sum() == 0:\n","            preds[b][8] = 1\n","\n","        # hate: 1, no_hate: 1\n","        if preds[b][:-1].sum() != 0:\n","            preds[b][8] = 0\n","\n","\n","\n","    accuracy = Accuracy(task='multilabel', num_labels=9)\n","    f1_macro = F1Score(task=\"multilabel\", num_labels=9, average='macro')\n","    f1_micro = F1Score(task=\"multilabel\", num_labels=9, average='micro')\n","    f1_weight = F1Score(task=\"multilabel\", num_labels=9, average='weighted')\n","    em = ExactMatch(task='multiclass', num_classes=2)\n","    auroc = AUROC(task='multilabel', num_labels=9, average='micro')\n","    hamming = HammingDistance(task=\"multiclass\", num_classes=2)\n","    n_int_accuracy = Accuracy(task='multiclass', num_classes=9)\n","\n","    f1s = F1Score(task='multilabel', num_labels=9, average=None)\n","\n","    print(\"f1s\")\n","    print(f1s(preds, intent_labels))\n","\n","\n","    metrics = {'accuracy': accuracy(preds, intent_labels),\n","               'f1_macro': f1_macro(preds, intent_labels),\n","               'f1_micro': f1_micro(preds, intent_labels),\n","               'f1_weighted': f1_weight(preds, intent_labels),\n","               'auroc': auroc(preds, intent_labels),\n","               'hamming_loss': hamming(preds, intent_labels),\n","               'em': em(preds, intent_labels),\n","               'n_int_accuracy': n_int_accuracy(n_pred, n_labels)}\n","    return metrics"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"hbxV9-G6FDM5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"]}],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_valid,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"qJbkJX5wCjMA"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24700' max='24700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24700/24700 2:11:11, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Macro</th>\n","      <th>F1 Micro</th>\n","      <th>F1 Weighted</th>\n","      <th>Auroc</th>\n","      <th>Hamming Loss</th>\n","      <th>Em</th>\n","      <th>N Int Accuracy</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","      <th>Steps Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.679000</td>\n","      <td>0.539966</td>\n","      <td>0.955877</td>\n","      <td>0.676458</td>\n","      <td>0.821803</td>\n","      <td>0.814926</td>\n","      <td>0.893281</td>\n","      <td>0.044123</td>\n","      <td>0.778829</td>\n","      <td>0.807885</td>\n","      <td>17.699000</td>\n","      <td>495.848000</td>\n","      <td>7.797000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.462600</td>\n","      <td>0.497409</td>\n","      <td>0.960967</td>\n","      <td>0.705162</td>\n","      <td>0.842856</td>\n","      <td>0.838032</td>\n","      <td>0.906199</td>\n","      <td>0.039033</td>\n","      <td>0.803783</td>\n","      <td>0.824521</td>\n","      <td>17.527200</td>\n","      <td>500.708000</td>\n","      <td>7.873000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.357000</td>\n","      <td>0.532821</td>\n","      <td>0.962689</td>\n","      <td>0.718005</td>\n","      <td>0.850565</td>\n","      <td>0.848101</td>\n","      <td>0.912317</td>\n","      <td>0.037311</td>\n","      <td>0.811645</td>\n","      <td>0.820761</td>\n","      <td>17.530300</td>\n","      <td>500.619000</td>\n","      <td>7.872000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.269100</td>\n","      <td>0.588268</td>\n","      <td>0.962296</td>\n","      <td>0.735227</td>\n","      <td>0.849733</td>\n","      <td>0.849171</td>\n","      <td>0.913517</td>\n","      <td>0.037704</td>\n","      <td>0.809708</td>\n","      <td>0.822015</td>\n","      <td>17.328600</td>\n","      <td>506.447000</td>\n","      <td>7.964000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.200100</td>\n","      <td>0.660823</td>\n","      <td>0.963765</td>\n","      <td>0.738455</td>\n","      <td>0.854912</td>\n","      <td>0.852096</td>\n","      <td>0.914874</td>\n","      <td>0.036235</td>\n","      <td>0.818368</td>\n","      <td>0.832270</td>\n","      <td>17.518700</td>\n","      <td>500.951000</td>\n","      <td>7.877000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.150200</td>\n","      <td>0.729456</td>\n","      <td>0.963486</td>\n","      <td>0.734646</td>\n","      <td>0.853693</td>\n","      <td>0.850195</td>\n","      <td>0.913938</td>\n","      <td>0.036514</td>\n","      <td>0.817229</td>\n","      <td>0.830902</td>\n","      <td>17.457800</td>\n","      <td>502.699000</td>\n","      <td>7.905000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.116400</td>\n","      <td>0.837791</td>\n","      <td>0.963474</td>\n","      <td>0.738159</td>\n","      <td>0.854005</td>\n","      <td>0.852296</td>\n","      <td>0.914966</td>\n","      <td>0.036526</td>\n","      <td>0.817229</td>\n","      <td>0.828851</td>\n","      <td>17.559600</td>\n","      <td>499.783000</td>\n","      <td>7.859000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.098300</td>\n","      <td>0.892970</td>\n","      <td>0.964600</td>\n","      <td>0.755259</td>\n","      <td>0.858473</td>\n","      <td>0.855813</td>\n","      <td>0.917423</td>\n","      <td>0.035400</td>\n","      <td>0.822242</td>\n","      <td>0.832726</td>\n","      <td>17.397400</td>\n","      <td>504.445000</td>\n","      <td>7.932000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.080400</td>\n","      <td>0.978396</td>\n","      <td>0.963651</td>\n","      <td>0.739123</td>\n","      <td>0.855707</td>\n","      <td>0.854600</td>\n","      <td>0.918303</td>\n","      <td>0.036349</td>\n","      <td>0.817912</td>\n","      <td>0.824635</td>\n","      <td>17.379800</td>\n","      <td>504.953000</td>\n","      <td>7.940000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.067900</td>\n","      <td>1.001551</td>\n","      <td>0.964626</td>\n","      <td>0.741220</td>\n","      <td>0.858273</td>\n","      <td>0.855778</td>\n","      <td>0.916574</td>\n","      <td>0.035374</td>\n","      <td>0.823382</td>\n","      <td>0.832954</td>\n","      <td>17.401200</td>\n","      <td>504.334000</td>\n","      <td>7.931000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.057000</td>\n","      <td>1.134241</td>\n","      <td>0.964094</td>\n","      <td>0.748770</td>\n","      <td>0.856984</td>\n","      <td>0.855810</td>\n","      <td>0.917866</td>\n","      <td>0.035906</td>\n","      <td>0.819850</td>\n","      <td>0.830219</td>\n","      <td>17.516900</td>\n","      <td>501.002000</td>\n","      <td>7.878000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.049700</td>\n","      <td>1.159742</td>\n","      <td>0.964436</td>\n","      <td>0.765769</td>\n","      <td>0.858224</td>\n","      <td>0.856760</td>\n","      <td>0.918277</td>\n","      <td>0.035564</td>\n","      <td>0.820989</td>\n","      <td>0.831928</td>\n","      <td>17.895600</td>\n","      <td>490.401000</td>\n","      <td>7.711000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.042600</td>\n","      <td>1.298007</td>\n","      <td>0.963993</td>\n","      <td>0.753387</td>\n","      <td>0.856436</td>\n","      <td>0.854779</td>\n","      <td>0.917204</td>\n","      <td>0.036007</td>\n","      <td>0.819850</td>\n","      <td>0.831700</td>\n","      <td>17.825600</td>\n","      <td>492.325000</td>\n","      <td>7.742000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.037100</td>\n","      <td>1.273659</td>\n","      <td>0.965094</td>\n","      <td>0.760203</td>\n","      <td>0.860975</td>\n","      <td>0.859841</td>\n","      <td>0.920164</td>\n","      <td>0.034906</td>\n","      <td>0.823724</td>\n","      <td>0.835119</td>\n","      <td>17.460500</td>\n","      <td>502.619000</td>\n","      <td>7.904000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.032400</td>\n","      <td>1.322647</td>\n","      <td>0.965107</td>\n","      <td>0.765716</td>\n","      <td>0.861005</td>\n","      <td>0.859861</td>\n","      <td>0.920128</td>\n","      <td>0.034893</td>\n","      <td>0.823952</td>\n","      <td>0.831586</td>\n","      <td>17.556000</td>\n","      <td>499.886000</td>\n","      <td>7.861000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.028000</td>\n","      <td>1.351206</td>\n","      <td>0.965195</td>\n","      <td>0.761562</td>\n","      <td>0.861253</td>\n","      <td>0.859520</td>\n","      <td>0.920006</td>\n","      <td>0.034805</td>\n","      <td>0.825661</td>\n","      <td>0.833523</td>\n","      <td>17.392700</td>\n","      <td>504.579000</td>\n","      <td>7.934000</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.024600</td>\n","      <td>1.382067</td>\n","      <td>0.965841</td>\n","      <td>0.770435</td>\n","      <td>0.863861</td>\n","      <td>0.862713</td>\n","      <td>0.921583</td>\n","      <td>0.034159</td>\n","      <td>0.828054</td>\n","      <td>0.832726</td>\n","      <td>17.466700</td>\n","      <td>502.441000</td>\n","      <td>7.901000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.020800</td>\n","      <td>1.409754</td>\n","      <td>0.965677</td>\n","      <td>0.769599</td>\n","      <td>0.863253</td>\n","      <td>0.861813</td>\n","      <td>0.921359</td>\n","      <td>0.034323</td>\n","      <td>0.827598</td>\n","      <td>0.834207</td>\n","      <td>17.362900</td>\n","      <td>505.447000</td>\n","      <td>7.948000</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.018200</td>\n","      <td>1.425474</td>\n","      <td>0.965474</td>\n","      <td>0.763943</td>\n","      <td>0.862474</td>\n","      <td>0.861284</td>\n","      <td>0.920985</td>\n","      <td>0.034526</td>\n","      <td>0.826459</td>\n","      <td>0.832384</td>\n","      <td>17.441300</td>\n","      <td>503.172000</td>\n","      <td>7.912000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.018600</td>\n","      <td>1.428414</td>\n","      <td>0.965753</td>\n","      <td>0.770117</td>\n","      <td>0.863666</td>\n","      <td>0.862619</td>\n","      <td>0.921878</td>\n","      <td>0.034247</td>\n","      <td>0.827484</td>\n","      <td>0.834321</td>\n","      <td>17.459600</td>\n","      <td>502.647000</td>\n","      <td>7.904000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["f1s\n","tensor([0.6778, 0.7123, 0.7760, 0.8509, 0.7552, 0.6363, 0.0000, 0.7953, 0.8843])\n","f1s\n","tensor([0.7479, 0.7475, 0.8117, 0.8806, 0.7748, 0.6678, 0.0000, 0.8226, 0.8935])\n","f1s\n","tensor([0.7602, 0.7742, 0.8146, 0.8860, 0.7966, 0.7118, 0.0000, 0.8207, 0.8979])\n","f1s\n","tensor([0.7651, 0.7762, 0.8175, 0.8860, 0.8032, 0.7265, 0.1429, 0.8041, 0.8955])\n","f1s\n","tensor([0.7630, 0.7873, 0.8121, 0.8893, 0.7911, 0.7235, 0.1481, 0.8303, 0.9012])\n","f1s\n","tensor([0.7509, 0.7857, 0.8187, 0.8922, 0.7859, 0.7094, 0.1429, 0.8261, 0.9002])\n","f1s\n","tensor([0.7582, 0.7911, 0.8233, 0.8918, 0.8086, 0.7175, 0.1379, 0.8166, 0.8984])\n","f1s\n","tensor([0.7579, 0.7877, 0.8277, 0.8954, 0.8124, 0.7135, 0.2667, 0.8327, 0.9034])\n","f1s\n","tensor([0.7717, 0.7925, 0.8292, 0.8904, 0.8039, 0.7226, 0.1333, 0.8082, 0.9002])\n","f1s\n","tensor([0.7647, 0.7936, 0.8222, 0.8973, 0.8007, 0.7193, 0.1379, 0.8321, 0.9030])\n","f1s\n","tensor([0.7623, 0.7994, 0.8245, 0.8926, 0.8147, 0.7359, 0.1875, 0.8223, 0.8997])\n","f1s\n","tensor([0.7713, 0.7954, 0.8197, 0.8947, 0.8179, 0.7280, 0.3226, 0.8417, 0.9006])\n","f1s\n","tensor([0.7612, 0.7907, 0.8268, 0.8988, 0.7967, 0.7280, 0.2581, 0.8205, 0.8998])\n","f1s\n","tensor([0.7690, 0.7971, 0.8375, 0.8968, 0.8210, 0.7317, 0.2581, 0.8276, 0.9030])\n","f1s\n","tensor([0.7703, 0.8000, 0.8323, 0.8963, 0.8189, 0.7401, 0.3030, 0.8281, 0.9024])\n","f1s\n","tensor([0.7690, 0.8006, 0.8318, 0.8967, 0.8148, 0.7310, 0.2581, 0.8489, 0.9031])\n","f1s\n","tensor([0.7758, 0.8026, 0.8347, 0.8981, 0.8211, 0.7451, 0.3030, 0.8489, 0.9045])\n","f1s\n","tensor([0.7731, 0.8029, 0.8367, 0.8991, 0.8209, 0.7364, 0.3125, 0.8410, 0.9038])\n","f1s\n","tensor([0.7739, 0.8040, 0.8326, 0.8992, 0.8159, 0.7417, 0.2581, 0.8470, 0.9031])\n","f1s\n","tensor([0.7755, 0.8049, 0.8318, 0.9014, 0.8206, 0.7427, 0.3030, 0.8470, 0.9041])\n"]},{"data":{"text/plain":["TrainOutput(global_step=24700, training_loss=0.14049505604423493, metrics={'train_runtime': 7873.2607, 'train_samples_per_second': 200.621, 'train_steps_per_second': 3.137, 'total_flos': 0.0, 'train_loss': 0.14049505604423493, 'epoch': 20.0})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"d2isIEV05mnB"},"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"5zQeEYdxzKDP"},"outputs":[],"source":["# trainer.save_model()\n","import os\n","torch.save(model.state_dict(), os.path.join(f\"{save_dir}\", \"model.pt\"))"]},{"cell_type":"markdown","metadata":{"id":"dr6FMPTmeXKt"},"source":["## 예측"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"6mMdVLTm7rUx","outputId":"e22b7a53-6266-48d9-f725-89b2bc0ac6b6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f98c84c50d0d4ca5aa0e3f77355f1e3a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21939 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Model(\n","  (model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(16424, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (n_classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (intent_loss): FocalLoss()\n","  (n_loss): CrossEntropyLoss()\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_test = test.map(tokenize_function, batched=True)\n","\n","state_dict = torch.load(f\"{save_dir}/model.pt\")\n","\n","model = Model()\n","model.load_state_dict(state_dict=state_dict)\n","model.to(device)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"B6ekYwMLooHm"},"outputs":[{"name":"stderr","output_type":"stream","text":["Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_test,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"u4SFg8R3otvF","outputId":"d9372bfa-ccd1-424b-b0ab-e25d74bb7252"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  1/343 : < :]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["f1s\n","tensor([0.8039, 0.7772, 0.8294, 0.9066, 0.8245, 0.7423, 0.3059, 0.8287, 0.9004])\n"]},{"data":{"text/plain":["{'eval_loss': 1.439549207687378,\n"," 'eval_accuracy': 0.9648216366767883,\n"," 'eval_f1_macro': 0.7687655687332153,\n"," 'eval_f1_micro': 0.8609854578971863,\n"," 'eval_f1_weighted': 0.8600044846534729,\n"," 'eval_auroc': 0.9197064638137817,\n"," 'eval_hamming_loss': 0.03517836332321167,\n"," 'eval_em': 0.8225534558296204,\n"," 'eval_n_int_accuracy': 0.832490086555481,\n"," 'eval_runtime': 42.7355,\n"," 'eval_samples_per_second': 513.368,\n"," 'eval_steps_per_second': 8.026}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["Qz98cJCasPgI"],"provenance":[{"file_id":"171KhS1_LVBtpAFd_kaT8lcrZmhcz5ehY","timestamp":1693358862334},{"file_id":"1_yQBkQObI4NWjCw7fzw-0PiXmJT1YUoy","timestamp":1669644928935}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0912b330f7354f9f95c60a87f8caf994":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a4e9fa141b34dddba9ce934bfb3133d","IPY_MODEL_ebe85e2f525747c2bab1059d40917cf6","IPY_MODEL_57342a2d55b74c3b9a69a1805f94889c"],"layout":"IPY_MODEL_5097178ceeaa4bfca166641758ad2fc4"}},"5097178ceeaa4bfca166641758ad2fc4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"538a4c1eed7e40d29234c7120a3761b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57342a2d55b74c3b9a69a1805f94889c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b26ac6c874384214a6e05402e16216af","placeholder":"​","style":"IPY_MODEL_cedd6f11236a49518a88510e7a81bd56","value":" 78977/78977 [00:27&lt;00:00, 3047.45 examples/s]"}},"6cf048b7a36a47c7a23f8eab7397d481":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4e9fa141b34dddba9ce934bfb3133d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cf048b7a36a47c7a23f8eab7397d481","placeholder":"​","style":"IPY_MODEL_538a4c1eed7e40d29234c7120a3761b6","value":"Map: 100%"}},"a537a5456f44480b8e94dad15ea64b11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b26ac6c874384214a6e05402e16216af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c92d88c8388541e29bedf12c03bc9bd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cedd6f11236a49518a88510e7a81bd56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebe85e2f525747c2bab1059d40917cf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92d88c8388541e29bedf12c03bc9bd7","max":78977,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a537a5456f44480b8e94dad15ea64b11","value":78977}}}}},"nbformat":4,"nbformat_minor":0}
