{"cells":[{"cell_type":"markdown","metadata":{"id":"t2rd2GYlnYGX"},"source":["# 한국어 혐오 발언 탐지"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20790,"status":"ok","timestamp":1693462464751,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"b4YkgbEocsAb","outputId":"d150072e-7b5c-4acd-a6db-6646ed24b2c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZaCnbT5Reaqf"},"source":["## 필요 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70293,"status":"ok","timestamp":1693473244783,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"680tRbdhebLg","outputId":"fd3d8939-98c9-4846-ea74-7e599c3dde87"},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Collecting accelerate\n","  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\n","  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.8.4)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: accelerate\n","Successfully installed accelerate-0.22.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers\n","!pip install datasets\n","!pip install torchmetrics\n","!pip install accelerate -U"]},{"cell_type":"markdown","metadata":{"id":"Qz98cJCasPgI"},"source":["## 데이터셋 로드"]},{"cell_type":"markdown","metadata":{"id":"qqO0w2cKsjN1"},"source":["- 학습, 검증, 테스트 데이터셋 준비\n","- 라벨 정보\n","\n","      class_label:\n","        names:\n","          0: origin\n","          1: physical\n","          2: politics\n","          3: profanity\n","          4: age\n","          5: gender\n","          6: race\n","          7: religion\n","          8: not_hate_speech"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IczFOCZZfvx","outputId":"8318274e-bf48-48df-eb13-d661c1d6b94d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset kmhas_korean_hate_speech (/root/.cache/huggingface/datasets/jeanlee___kmhas_korean_hate_speech/default/1.0.0/17406fbed45548c92e0795df0675e21fb2a09ceaa098bd5ff58c7fdc7f8a63d4)\n","Found cached dataset kmhas_korean_hate_speech (/root/.cache/huggingface/datasets/jeanlee___kmhas_korean_hate_speech/default/1.0.0/17406fbed45548c92e0795df0675e21fb2a09ceaa098bd5ff58c7fdc7f8a63d4)\n","Found cached dataset kmhas_korean_hate_speech (/root/.cache/huggingface/datasets/jeanlee___kmhas_korean_hate_speech/default/1.0.0/17406fbed45548c92e0795df0675e21fb2a09ceaa098bd5ff58c7fdc7f8a63d4)\n"]}],"source":["from datasets import load_dataset\n","\n","train = load_dataset(\"jeanlee/kmhas_korean_hate_speech\", split=\"train\")\n","validation = load_dataset(\"jeanlee/kmhas_korean_hate_speech\", split=\"validation\")\n","test = load_dataset(\"jeanlee/kmhas_korean_hate_speech\", split=\"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1853,"status":"ok","timestamp":1693473259520,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"wqHX7ImG54T6","outputId":"b927813c-2149-4d8c-d80d-df30f9049ac2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 78977\n","})\n","Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 8776\n","})\n","Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 21939\n","})\n","\"자한당틀딱들.. 악플질 고만해라.\"\n","[2, 4]\n"]}],"source":["# 데이터 예제 출력\n","\n","print(train)\n","print(validation)\n","print(test)\n","print(train['text'][0])\n","print(train['label'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1693473259520,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"eo9Y7N2CsXC6","outputId":"5173db4e-a0d2-4a99-c38a-9dd786893b2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\"자한당틀딱들.. 악플질 고만해라.\"', '정치적으로 편향된 평론한은 분은 별로...', '적당히좀 쳐먹지.그랬냐??? 안그래도 문재인 때문에 나라 엉망진창인데...', '\"안서는 아재들 풀발기 ㅋㄲㅋ\"', '우와 ㅋ 능력자', '맛녀석 콩트보다 약했음맛녀석 애청자로써 70%실력발휘', '주영훈 솔직히 호감임 잉꼬부부로 소문났잖아', '이게주간아이돌이랑머가달라...', '아오 슈박 회사생활도 졑깥고 돈벌기 힘들어 죽겠구만 뭔 저딴것들 자꾸 tv나와서 사람 짜증나게하냐 외국서 편히살려면 아닥하고 살아라 대한민국서 취미로 돈벌어가지말고 좀 끄지라고!', '\"문재인 하는게 뭐 별거있냐?ㅂㅅㅅㅋ가 하는짓인데 어련하겠어.ㅋㅋㅋ\"']\n","{'text': ['\"자한당틀딱들.. 악플질 고만해라.\"', '정치적으로 편향된 평론한은 분은 별로...', '적당히좀 쳐먹지.그랬냐??? 안그래도 문재인 때문에 나라 엉망진창인데...', '\"안서는 아재들 풀발기 ㅋㄲㅋ\"', '우와 ㅋ 능력자', '맛녀석 콩트보다 약했음맛녀석 애청자로써 70%실력발휘', '주영훈 솔직히 호감임 잉꼬부부로 소문났잖아', '이게주간아이돌이랑머가달라...', '아오 슈박 회사생활도 졑깥고 돈벌기 힘들어 죽겠구만 뭔 저딴것들 자꾸 tv나와서 사람 짜증나게하냐 외국서 편히살려면 아닥하고 살아라 대한민국서 취미로 돈벌어가지말고 좀 끄지라고!', '\"문재인 하는게 뭐 별거있냐?ㅂㅅㅅㅋ가 하는짓인데 어련하겠어.ㅋㅋㅋ\"'], 'label': [[2, 4], [8], [2], [4], [8], [8], [8], [8], [3], [2, 3]]}\n"]}],"source":["# 사전\n","print(train['text'][0:10])\n","\n","# 리스트\n","print(train[0:10])\n"]},{"cell_type":"markdown","metadata":{"id":"R_2uWy1p8Ifx"},"source":["## 모델 및 토크나이저 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":5145,"status":"error","timestamp":1693917591046,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"jnbQNuzk7rZR","outputId":"f19e3aee-c3c4-482b-b33b-27332c27cd11"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n","  )\n",")"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","name_cards= ['monologg/koelectra-base-v3-discriminator', 'kobert-base-v1', 'bert-base-multilingual-cased']\n","name_card = \"monologg/koelectra-base-v3-discriminator\"\n","num_labels = 9\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(name_card, do_lower_case=False)\n","model = AutoModelForSequenceClassification.from_pretrained(name_card, num_labels=num_labels, problem_type=\"multi_label_classification\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTUT0OSy5mm-","outputId":"e7ba0517-c6a4-4343-e995-f6ff01914639"},"outputs":[{"data":{"text/plain":["Model(\n","  (model): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (n_classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (intent_loss): FocalLoss()\n","  (n_loss): CrossEntropyLoss()\n",")"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn as nn\n","from transformers import AutoModel\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","        return F_loss.mean()\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.model = AutoModel.from_pretrained(name_card)\n","\n","        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n","        self.n_classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n","\n","        # self.intent_loss = nn.BCEWithLogitsLoss()\n","        self.intent_loss = FocalLoss()\n","        self.n_loss = nn.CrossEntropyLoss()\n","\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None, n_label=None):\n","        model_input= {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","        }\n","\n","        if token_type_ids is not None:\n","            model_input['token_type_ids'] = token_type_ids\n","\n","        hidden = self.model(**model_input)\n","\n","        # some output dont have pooler_output\n","        if hasattr(hidden, 'pooler_output'):\n","            cls = hidden.pooler_output\n","        else:\n","            cls = hidden.last_hidden_state[:, 0, :]\n","\n","        hidden = hidden.last_hidden_state[:, 1:, :]\n","\n","        intent_logit = self.classifier(hidden.mean(dim=1))\n","        n_logit = self.n_classifier(cls)\n","\n","        intent_loss = self.intent_loss(intent_logit, labels)\n","        num_loss = self.n_loss(n_logit, n_label)\n","\n","        loss = intent_loss + num_loss\n","\n","        model_output = {\n","            'loss': loss,\n","            'intent_logit': intent_logit,\n","            'n_logit': n_logit,\n","        }\n","\n","        return model_output\n","\n","\n","model = Model()\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1693464855626,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"67p9XNOm-K12","outputId":"713fe8a9-7638-403f-86bf-f7920399f60d"},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', '\"', '자', '##한', '##당', '##틀', '##딱', '##들', '.', '.', '악', '##플', '##질', '고만', '##해', '##라', '.', '\"', '[SEP]']\n","[2, 6, 3254, 4283, 4403, 4882, 5136, 4006, 18, 18, 3080, 4711, 4152, 25141, 4151, 4118, 18, 6, 3]\n","{'input_ids': [2, 6, 3254, 4283, 4403, 4882, 5136, 4006, 18, 18, 3080, 4711, 4152, 25141, 4151, 4118, 18, 6, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["# 토크나이저 예시\n","\n","ids = tokenizer.encode(train['text'][0])\n","tokenized_words = tokenizer.convert_ids_to_tokens(ids)\n","model_input = tokenizer(train['text'][0])\n","\n","print(tokenized_words)\n","print(ids)\n","print(model_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1693465554951,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"ig5R7bPUw8br","outputId":"0c8e4d33-c42f-47fe-814b-55bb8a96115f"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [2, 6, 3254, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n","['[CLS]', '\"', '자', '[SEP]']\n"]}],"source":["# 토크나이저 예시2\n","\n","model_input = tokenizer(train['text'][0], max_length=4, truncation=True, padding=\"max_length\")\n","print(model_input)\n","reverse = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n","print(reverse)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1298,"status":"ok","timestamp":1693465557105,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"PN25M6wXxhus","outputId":"1d5b61c5-3fdb-4010-8a5b-42595cf9db93"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [2, 6, 3254, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n","{'input_ids': [0, 113, 43998, 2], 'attention_mask': [1, 1, 1, 1]}\n","{'input_ids': [101, 1000, 100, 102], 'attention_mask': [1, 1, 1, 1]}\n"]}],"source":["# 토크나이저 예시3\n","\n","print(model_input)\n","\n","another_name_card = 'roberta-base'\n","another_tokenizer = AutoTokenizer.from_pretrained(another_name_card, do_lower_case=False)\n","model_input = another_tokenizer(train['text'][0], max_length=4, truncation=True, padding=\"max_length\")\n","print(model_input)\n","\n","another_name_card = 'distilbert-base-uncased'\n","another_tokenizer = AutoTokenizer.from_pretrained(another_name_card, do_lower_case=False)\n","model_input = another_tokenizer(train['text'][0], max_length=4, truncation=True, padding=\"max_length\")\n","print(model_input)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m9abTda-TIX"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def tokenize_function(examples):\n","    model_input = tokenizer(examples['text'], max_length=128, truncation=True, padding=\"max_length\")\n","\n","    mlb = MultiLabelBinarizer(classes=[0,1,2,3,4,5,6,7,8])\n","    one_hot_labels = mlb.fit_transform(examples['label'])\n","\n","    model_input['label'] = one_hot_labels\n","    # model_input['n_label'] = []\n","    # for one_hot_label in one_hot_labels:\n","    #     if one_hot_label[-1] == 0:\n","    #         model_input['n_label'].append(sum(one_hot_label) - 1)\n","    #     else:\n","    #         model_input['n_label'].append(0)\n","    model_input['n_label'] = [sum(one_hot_label[:-1]) for one_hot_label in one_hot_labels] # 이거도 가능할 듯\n","    # model_input['n_label'] = [sum(one_hot_label) - 1 for one_hot_label in one_hot_labels]\n","\n","    return model_input"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195,"referenced_widgets":["0912b330f7354f9f95c60a87f8caf994","8a4e9fa141b34dddba9ce934bfb3133d","ebe85e2f525747c2bab1059d40917cf6","57342a2d55b74c3b9a69a1805f94889c","5097178ceeaa4bfca166641758ad2fc4","6cf048b7a36a47c7a23f8eab7397d481","538a4c1eed7e40d29234c7120a3761b6","c92d88c8388541e29bedf12c03bc9bd7","a537a5456f44480b8e94dad15ea64b11","b26ac6c874384214a6e05402e16216af","cedd6f11236a49518a88510e7a81bd56"]},"executionInfo":{"elapsed":28200,"status":"ok","timestamp":1693473289678,"user":{"displayName":"조성민","userId":"07022805073498996704"},"user_tz":-540},"id":"OjnfLtIVIrIW","outputId":"7de4c6a9-bf5a-4e20-f345-89653d5262ef"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/jeanlee___kmhas_korean_hate_speech/default/1.0.0/17406fbed45548c92e0795df0675e21fb2a09ceaa098bd5ff58c7fdc7f8a63d4/cache-aeaed75530145819.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/jeanlee___kmhas_korean_hate_speech/default/1.0.0/17406fbed45548c92e0795df0675e21fb2a09ceaa098bd5ff58c7fdc7f8a63d4/cache-1e873c73a9686b7f.arrow\n"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 78977\n","})\n","Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask', 'n_label'],\n","    num_rows: 78977\n","})\n"]}],"source":["print(train)\n","\n","tokenized_train = train.map(tokenize_function, batched=True)\n","tokenized_valid = validation.map(tokenize_function, batched=True)\n","\n","print(tokenized_train)"]},{"cell_type":"markdown","metadata":{"id":"61-IoYan9kAq"},"source":["## 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WqZr7Mz7rXD"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","BATCH_SIZE = 32 # 32\n","EPOCHS = 20\n","\n","save_dir = '/root/model'\n","\n","training_args = TrainingArguments(\n","    output_dir=save_dir,\n","    do_train=True,\n","    do_eval=True,\n","    save_steps=999999999,\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    logging_strategy=\"epoch\",\n","    logging_dir='./logs',\n","    learning_rate=2e-5,\n","    run_name=\"v1\",\n","    label_names=['labels', 'n_label'],\n","    seed=42,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRE8aPEzQPrL"},"outputs":[],"source":["import torch\n","from torchmetrics import Accuracy, F1Score, HammingDistance, AUROC, ExactMatch\n","\n","def compute_metrics(eval_pred):\n","    threshold = 0.5\n","\n","    # model_output = eval_pred.predictions\n","    # labels = eval_pred.label_ids\n","    intent_logits, n_logits = eval_pred.predictions\n","    intent_labels, n_labels = eval_pred.label_ids\n","\n","\n","    intent_logits = torch.Tensor(intent_logits)\n","    intent_labels = torch.Tensor(intent_labels).long()\n","\n","    n_logits = torch.Tensor(n_logits)\n","    n_labels = torch.Tensor(n_labels).long()\n","\n","    n_pred = torch.argmax(n_logits, dim=1)\n","\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(intent_logits))\n","\n","    preds = torch.zeros(size = probs.size())\n","    preds[probs >= threshold] = 1\n","\n","    ...\n","\n","    for b in range(preds.shape[0]):\n","\n","        # hate: 1, no_hate: 1\n","        if preds[b][8] == 1:\n","            preds[b][:-1] = 0\n","\n","        # hate: 0, no_hate: 0\n","        if preds[b][:-1].sum() == 0:\n","            preds[b][8] = 1\n","\n","        # hate: 1, no_hate: 1\n","        if preds[b][:-1].sum() != 0:\n","            preds[b][8] = 0\n","\n","\n","\n","    accuracy = Accuracy(task='multilabel', num_labels=9)\n","    f1_macro = F1Score(task=\"multilabel\", num_labels=9, average='macro')\n","    f1_micro = F1Score(task=\"multilabel\", num_labels=9, average='micro')\n","    f1_weight = F1Score(task=\"multilabel\", num_labels=9, average='weighted')\n","    em = ExactMatch(task='multiclass', num_classes=2)\n","    auroc = AUROC(task='multilabel', num_labels=9, average='micro')\n","    hamming = HammingDistance(task=\"multiclass\", num_classes=2)\n","    n_int_accuracy = Accuracy(task='multiclass', num_classes=9)\n","\n","    f1s = F1Score(task='multilabel', num_labels=9, average=None)\n","\n","    print(\"f1s\")\n","    print(f1s(preds, intent_labels))\n","\n","\n","    metrics = {'accuracy': accuracy(preds, intent_labels),\n","               'f1_macro': f1_macro(preds, intent_labels),\n","               'f1_micro': f1_micro(preds, intent_labels),\n","               'f1_weighted': f1_weight(preds, intent_labels),\n","               'auroc': auroc(preds, intent_labels),\n","               'hamming_loss': hamming(preds, intent_labels),\n","               'em': em(preds, intent_labels),\n","               'n_int_accuracy': n_int_accuracy(n_pred, n_labels)}\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbxV9-G6FDM5"},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_valid,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJbkJX5wCjMA"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"d2isIEV05mnB"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zQeEYdxzKDP"},"outputs":[],"source":["trainer.save_model()"]},{"cell_type":"markdown","metadata":{"id":"dr6FMPTmeXKt"},"source":["## 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mMdVLTm7rUx","outputId":"e22b7a53-6266-48d9-f725-89b2bc0ac6b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/jeanlee___kmhas_korean_hate_speech/default/1.0.0/17406fbed45548c92e0795df0675e21fb2a09ceaa098bd5ff58c7fdc7f8a63d4/cache-67f51af271de2638.arrow\n"]},{"data":{"text/plain":["Model(\n","  (model): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (n_classifier): Linear(in_features=768, out_features=9, bias=True)\n","  (intent_loss): FocalLoss()\n","  (n_loss): CrossEntropyLoss()\n",")"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_test = test.map(tokenize_function, batched=True)\n","\n","state_dict = torch.load(f\"{save_dir}/pytorch_model.bin\")\n","\n","model = Model()\n","model.load_state_dict(state_dict=state_dict)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6ekYwMLooHm"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_test,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4SFg8R3otvF","outputId":"d9372bfa-ccd1-424b-b0ab-e25d74bb7252"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='686' max='686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [686/686 00:38]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["f1s\n","tensor([0.7986, 0.7862, 0.8306, 0.8918, 0.8355, 0.7483, 0.1818, 0.8415, 0.9047])\n"]},{"data":{"text/plain":["{'eval_loss': 1.4660391807556152,\n"," 'eval_accuracy': 0.9650546312332153,\n"," 'eval_f1_macro': 0.757649302482605,\n"," 'eval_f1_micro': 0.8622259497642517,\n"," 'eval_f1_weighted': 0.8613871932029724,\n"," 'eval_auroc': 0.9212228059768677,\n"," 'eval_hamming_loss': 0.03494536876678467,\n"," 'eval_em': 0.8222343921661377,\n"," 'eval_n_int_accuracy': 0.8323533535003662,\n"," 'eval_runtime': 39.4324,\n"," 'eval_samples_per_second': 556.369,\n"," 'eval_steps_per_second': 17.397}"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]}],"metadata":{"colab":{"collapsed_sections":["Qz98cJCasPgI"],"provenance":[{"file_id":"171KhS1_LVBtpAFd_kaT8lcrZmhcz5ehY","timestamp":1693358862334},{"file_id":"1_yQBkQObI4NWjCw7fzw-0PiXmJT1YUoy","timestamp":1669644928935}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0912b330f7354f9f95c60a87f8caf994":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a4e9fa141b34dddba9ce934bfb3133d","IPY_MODEL_ebe85e2f525747c2bab1059d40917cf6","IPY_MODEL_57342a2d55b74c3b9a69a1805f94889c"],"layout":"IPY_MODEL_5097178ceeaa4bfca166641758ad2fc4"}},"5097178ceeaa4bfca166641758ad2fc4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"538a4c1eed7e40d29234c7120a3761b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57342a2d55b74c3b9a69a1805f94889c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b26ac6c874384214a6e05402e16216af","placeholder":"​","style":"IPY_MODEL_cedd6f11236a49518a88510e7a81bd56","value":" 78977/78977 [00:27&lt;00:00, 3047.45 examples/s]"}},"6cf048b7a36a47c7a23f8eab7397d481":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4e9fa141b34dddba9ce934bfb3133d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cf048b7a36a47c7a23f8eab7397d481","placeholder":"​","style":"IPY_MODEL_538a4c1eed7e40d29234c7120a3761b6","value":"Map: 100%"}},"a537a5456f44480b8e94dad15ea64b11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b26ac6c874384214a6e05402e16216af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c92d88c8388541e29bedf12c03bc9bd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cedd6f11236a49518a88510e7a81bd56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebe85e2f525747c2bab1059d40917cf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92d88c8388541e29bedf12c03bc9bd7","max":78977,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a537a5456f44480b8e94dad15ea64b11","value":78977}}}}},"nbformat":4,"nbformat_minor":0}